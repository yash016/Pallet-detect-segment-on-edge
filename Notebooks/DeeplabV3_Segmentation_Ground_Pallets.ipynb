{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Semantic Segmentation of Ground and Pallets Using DeepLabV3"
      ],
      "metadata": {
        "id": "ja6F-PEx0PUI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installs"
      ],
      "metadata": {
        "id": "GjrVSmb04Zm_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install segmentation_models_pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGoWhco34gQK",
        "outputId": "6f23730c-ff01-48ec-a6c5-2b9c84648c08"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting segmentation_models_pytorch\n",
            "  Downloading segmentation_models_pytorch-0.3.4-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting efficientnet-pytorch==0.7.1 (from segmentation_models_pytorch)\n",
            "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: huggingface-hub>=0.24.6 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (0.24.7)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (10.4.0)\n",
            "Collecting pretrainedmodels==0.7.4 (from segmentation_models_pytorch)\n",
            "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (1.16.0)\n",
            "Collecting timm==0.9.7 (from segmentation_models_pytorch)\n",
            "  Downloading timm-0.9.7-py3-none-any.whl.metadata (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (0.20.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (4.66.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.5.0+cu121)\n",
            "Collecting munch (from pretrainedmodels==0.7.4->segmentation_models_pytorch)\n",
            "  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm==0.9.7->segmentation_models_pytorch) (6.0.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm==0.9.7->segmentation_models_pytorch) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation_models_pytorch) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation_models_pytorch) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation_models_pytorch) (24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation_models_pytorch) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation_models_pytorch) (4.12.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (1.26.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation_models_pytorch) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation_models_pytorch) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation_models_pytorch) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation_models_pytorch) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.0.2)\n",
            "Downloading segmentation_models_pytorch-0.3.4-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading timm-0.9.7-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n",
            "Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16424 sha256=6000b89529891fb1e6f428e09ce8ab4e3efaf27d009a2cd808841ea209edb7ba\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60944 sha256=00a5865ce9807c887b2f6a1021df6fe802fa12d5a246675a797a4934f40cc1fb\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/cb/a5/8f534c60142835bfc889f9a482e4a67e0b817032d9c6883b64\n",
            "Successfully built efficientnet-pytorch pretrainedmodels\n",
            "Installing collected packages: munch, efficientnet-pytorch, timm, pretrainedmodels, segmentation_models_pytorch\n",
            "  Attempting uninstall: timm\n",
            "    Found existing installation: timm 1.0.11\n",
            "    Uninstalling timm-1.0.11:\n",
            "      Successfully uninstalled timm-1.0.11\n",
            "Successfully installed efficientnet-pytorch-0.7.1 munch-4.0.0 pretrainedmodels-0.7.4 segmentation_models_pytorch-0.3.4 timm-0.9.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports\n"
      ],
      "metadata": {
        "id": "NKb1AVQmUn7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import copy\n",
        "import time\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import Compose, Resize, ToTensor, Normalize, InterpolationMode, ColorJitter, RandomResizedCrop, RandomAffine, RandomHorizontalFlip, RandomVerticalFlip, RandomRotation\n",
        "from torch.amp import autocast, GradScaler\n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "# ============================\n",
        "# 1. Set Random Seed for Reproducibility\n",
        "# ============================\n",
        "\n",
        "random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)"
      ],
      "metadata": {
        "id": "PmneorPFvSb5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# 2. Define Paths\n",
        "# ============================\n",
        "\n",
        "HOME_DIR = \"/content/drive/MyDrive/Pallets_detection\"\n",
        "SEG_DATASET_DIR = os.path.join(HOME_DIR, \"Deeplabv3_ObjectSegmentation_Dataset\")\n",
        "\n",
        "train_images_dir = os.path.join(SEG_DATASET_DIR, \"train\", \"images\")\n",
        "train_masks_dir = os.path.join(SEG_DATASET_DIR, \"train\", \"masks\")\n",
        "val_images_dir = os.path.join(SEG_DATASET_DIR, \"val\", \"images\")\n",
        "val_masks_dir = os.path.join(SEG_DATASET_DIR, \"val\", \"masks\")\n",
        "test_images_dir = os.path.join(SEG_DATASET_DIR, \"test\", \"images\")\n",
        "test_masks_dir = os.path.join(SEG_DATASET_DIR, \"test\", \"masks\")"
      ],
      "metadata": {
        "id": "TcgIOfMsvWlh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Define map_mask Function\n",
        "This function converts specific pixel values in masks to class labels, standardizing mask data to enable effective training.\n",
        "\n",
        "### 4. Define Custom Dataset Class\n",
        "Here, we create a custom dataset class to load images and corresponding masks, applying transformations and returning data in a compatible format for training."
      ],
      "metadata": {
        "id": "Pw5prsYZz7GF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# 3. Define map_mask Function\n",
        "# ============================\n",
        "\n",
        "def map_mask(mask):\n",
        "    \"\"\"\n",
        "    Maps specific pixel values in the mask to class indices.\n",
        "    - 0 (Background) -> 0\n",
        "    - 128 (Ground) -> 1\n",
        "    - 255 (Pallet) -> 2\n",
        "    \"\"\"\n",
        "    mask = np.array(mask)\n",
        "    mask = np.where(mask == 128, 1, mask)  # Ground -> 1\n",
        "    mask = np.where(mask == 255, 2, mask)  # Pallet -> 2\n",
        "    mask = np.where((mask != 1) & (mask != 2), 0, mask)  # Background -> 0\n",
        "    return mask\n",
        "\n",
        "# ============================\n",
        "# 4. Define Custom Dataset Class\n",
        "# ============================\n",
        "\n",
        "class PalletDataset(Dataset):\n",
        "    def __init__(self, images_dir, masks_dir, image_transform=None, mask_transform=None):\n",
        "        \"\"\"\n",
        "        Custom Dataset for Semantic Segmentation.\n",
        "\n",
        "        Args:\n",
        "            images_dir (str): Directory containing input images.\n",
        "            masks_dir (str): Directory containing corresponding masks.\n",
        "            image_transform (callable, optional): Transformations to apply to images.\n",
        "            mask_transform (callable, optional): Transformations to apply to masks.\n",
        "        \"\"\"\n",
        "        self.images_dir = images_dir\n",
        "        self.masks_dir = masks_dir\n",
        "        self.image_transform = image_transform\n",
        "        self.mask_transform = mask_transform\n",
        "        self.image_files = [f for f in os.listdir(images_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_files[idx]\n",
        "        img_path = os.path.join(self.images_dir, img_name)\n",
        "        mask_name = os.path.splitext(img_name)[0] + \".png\"\n",
        "        mask_path = os.path.join(self.masks_dir, mask_name)\n",
        "\n",
        "        if not os.path.exists(mask_path):\n",
        "            raise FileNotFoundError(f\"Mask file {mask_path} does not exist for image {img_name}\")\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        mask = Image.open(mask_path).convert(\"L\")\n",
        "\n",
        "        if self.image_transform and self.mask_transform:\n",
        "            # Apply joint transformations using a seed to ensure consistency\n",
        "            seed = np.random.randint(21)  # Make a seed with numpy generator\n",
        "\n",
        "            random.seed(seed)\n",
        "            torch.manual_seed(seed)\n",
        "            image = self.image_transform(image)\n",
        "\n",
        "            random.seed(seed)\n",
        "            torch.manual_seed(seed)\n",
        "            mask = self.mask_transform(mask)\n",
        "        else:\n",
        "            if self.image_transform:\n",
        "                image = self.image_transform(image)\n",
        "            if self.mask_transform:\n",
        "                mask = self.mask_transform(mask)\n",
        "\n",
        "        mask = map_mask(mask)\n",
        "        mask = torch.from_numpy(mask).long()\n",
        "        return image, mask"
      ],
      "metadata": {
        "id": "0mjru9Kyvm_R"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Define Transformations\n",
        "Transformations are applied to images and masks to augment the data. This includes resizing, flipping, and adjusting brightness, which helps prevent overfitting by diversifying the training data.\n",
        "\n",
        "### 6. Initialize Datasets and DataLoaders\n",
        "We initialize datasets and loaders for training, validation, and testing, enabling efficient and parallelized data loading during training.\n"
      ],
      "metadata": {
        "id": "oTpiqfZ90Mnj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# 5. Define Transformations\n",
        "# ============================\n",
        "\n",
        "input_size = 512\n",
        "\n",
        "# Image transformations for training with advanced augmentations\n",
        "from torchvision.transforms import ColorJitter, RandomResizedCrop, RandomAffine\n",
        "\n",
        "image_transform = Compose([\n",
        "    RandomResizedCrop(input_size, scale=(0.8, 1.0), ratio=(0.9, 1.1)),\n",
        "    RandomHorizontalFlip(),\n",
        "    RandomVerticalFlip(),\n",
        "    RandomRotation(10),\n",
        "    ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    ToTensor(),\n",
        "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "mask_transform = Compose([\n",
        "    RandomResizedCrop(input_size, scale=(0.8, 1.0), ratio=(0.9, 1.1), interpolation=InterpolationMode.NEAREST),\n",
        "    RandomHorizontalFlip(),\n",
        "    RandomVerticalFlip(),\n",
        "    RandomRotation(10, interpolation=InterpolationMode.NEAREST),\n",
        "])\n",
        "\n",
        "# Image transformations for validation and testing (no augmentation)\n",
        "val_test_image_transform = Compose([\n",
        "    Resize((input_size, input_size), interpolation=InterpolationMode.BILINEAR),\n",
        "    ToTensor(),\n",
        "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Mask transformations for validation and testing (resize only)\n",
        "val_test_mask_transform = Compose([\n",
        "    Resize((input_size, input_size), interpolation=InterpolationMode.NEAREST)\n",
        "])\n",
        "\n",
        "# ============================\n",
        "# 6. Initialize Datasets and DataLoaders\n",
        "# ============================\n",
        "\n",
        "batch_size = 8  # Increased batch size for better batch statistics\n",
        "num_workers = 4\n",
        "\n",
        "train_dataset = PalletDataset(\n",
        "    images_dir=train_images_dir,\n",
        "    masks_dir=train_masks_dir,\n",
        "    image_transform=image_transform,\n",
        "    mask_transform=mask_transform\n",
        ")\n",
        "\n",
        "val_dataset = PalletDataset(\n",
        "    images_dir=val_images_dir,\n",
        "    masks_dir=val_masks_dir,\n",
        "    image_transform=val_test_image_transform,\n",
        "    mask_transform=val_test_mask_transform\n",
        ")\n",
        "\n",
        "test_dataset = PalletDataset(\n",
        "    images_dir=test_images_dir,\n",
        "    masks_dir=test_masks_dir,\n",
        "    image_transform=val_test_image_transform,\n",
        "    mask_transform=val_test_mask_transform\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=False\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    num_workers=num_workers\n",
        ")\n",
        "\n",
        "def plot_class_distribution(dataloader, num_classes=3):\n",
        "    class_counts = np.zeros(num_classes)\n",
        "    for _, masks in dataloader:\n",
        "        masks = masks.numpy()\n",
        "        for cls in range(num_classes):\n",
        "            class_counts[cls] += np.sum(masks == cls)\n",
        "\n",
        "    plt.bar(range(num_classes), class_counts, tick_label=['Background', 'Ground', 'Pallet'])\n",
        "    plt.xlabel('Classes')\n",
        "    plt.ylabel('Pixel Count')\n",
        "    plt.title('Class Distribution')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "GHoYiNnJvuhC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_class_distribution(dataloader, num_classes=3):\n",
        "    class_counts = np.zeros(num_classes)\n",
        "    for _, masks in dataloader:\n",
        "        masks = masks.numpy()\n",
        "        for cls in range(num_classes):\n",
        "            class_counts[cls] += np.sum(masks == cls)\n",
        "\n",
        "    plt.bar(range(num_classes), class_counts, tick_label=['Background', 'Ground', 'Pallet'])\n",
        "    plt.xlabel('Classes')\n",
        "    plt.ylabel('Pixel Count')\n",
        "    plt.title('Class Distribution')\n",
        "    plt.show()\n",
        "\n",
        "# Plot distribution for training set\n",
        "plot_class_distribution(train_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "Li2K5YGSiT0L",
        "outputId": "d3ce53d7-9e7e-4007-c037-1de9bb156a82"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Define Model (DeepLabV3+)\n",
        "Here, we define the **DeepLabV3+** model, a popular architecture for semantic segmentation. It’s chosen for its accuracy in segmenting complex objects, with **ResNet-101** as the backbone for a strong feature extraction base."
      ],
      "metadata": {
        "id": "EdAb57U64ACD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# 7. Define Model (DeepLabV3+)\n",
        "# ============================\n",
        "\n",
        "\n",
        "# Create DeepLabV3+ model\n",
        "def createDeepLabv3Plus(outputchannels=3, encoder_name='resnet101', encoder_weights='imagenet'):\n",
        "    \"\"\"\n",
        "    Creates a DeepLabV3+ model using segmentation_models_pytorch.\n",
        "\n",
        "    Args:\n",
        "        outputchannels (int): Number of output classes.\n",
        "        encoder_name (str): Name of the encoder backbone.\n",
        "        encoder_weights (str): Pretrained weights for the encoder.\n",
        "\n",
        "    Returns:\n",
        "        model (torch.nn.Module): DeepLabV3+ model.\n",
        "    \"\"\"\n",
        "    model = smp.DeepLabV3Plus(\n",
        "        encoder_name=encoder_name,\n",
        "        encoder_weights=encoder_weights,\n",
        "        in_channels=3,\n",
        "        classes=outputchannels,\n",
        "        activation=None\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Instantiate the model\n",
        "model = createDeepLabv3Plus(outputchannels=3, encoder_name='resnet101', encoder_weights='imagenet')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "YbmD3Y3ev3AA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Define Loss, Optimizer, and Scheduler\n",
        "In this section, we define:\n",
        "- **Combined Loss (CrossEntropy + Dice Loss)**: CrossEntropy Loss handles classification, while Dice Loss addresses class imbalance and improves boundary accuracy, particularly useful in segmentation tasks with smaller classes.\n",
        "- **Optimizer (AdamW)**: AdamW helps manage the model’s weight decay and is effective for segmentation due to its stability and efficiency.\n",
        "- **Scheduler (CosineAnnealingWarmRestarts)**: The Cosine Annealing scheduler with warm restarts adjusts the learning rate dynamically, encouraging better convergence over multiple cycles.\n"
      ],
      "metadata": {
        "id": "YdnQTKa34yBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# 8. Define Loss, Optimizer, and Scheduler\n",
        "# ============================\n",
        "\n",
        "# Adjusted class weights using median frequency balancing\n",
        "def calculate_class_weights(dataloader, num_classes=3):\n",
        "    class_counts = np.zeros(num_classes)\n",
        "    total_counts = 0\n",
        "    for _, masks in dataloader:\n",
        "        masks = masks.cpu().numpy()\n",
        "        total_counts += masks.size\n",
        "        for cls in range(num_classes):\n",
        "            class_counts[cls] += np.sum(masks == cls)\n",
        "    class_freqs = class_counts / total_counts\n",
        "    median_freq = np.median(class_freqs)\n",
        "    class_weights = median_freq / (class_freqs + 1e-6)  # Add small value to avoid division by zero\n",
        "    return torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "\n",
        "dataloaders = {'Train': train_loader, 'Val': val_loader}\n",
        "\n",
        "class_weights = calculate_class_weights(dataloaders['Train'], num_classes=3)\n",
        "print(f'Class weights: {class_weights}')\n",
        "print(f'Class weights shape: {class_weights.shape}')  # Should output: torch.Size([3])\n",
        "\n",
        "# Use a combination of CrossEntropyLoss and DiceLoss\n",
        "class DiceLossCustom(nn.Module):\n",
        "    def __init__(self, weight=None, ignore_index=None):\n",
        "        super(DiceLossCustom, self).__init__()\n",
        "        self.weight = weight\n",
        "        self.ignore_index = ignore_index\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        inputs = torch.softmax(inputs, dim=1)\n",
        "        targets_one_hot = nn.functional.one_hot(targets, num_classes=inputs.size(1)).permute(0, 3, 1, 2).float()\n",
        "        dims = (0, 2, 3)\n",
        "        intersection = torch.sum(inputs * targets_one_hot, dims)\n",
        "        cardinality = torch.sum(inputs + targets_one_hot, dims)\n",
        "        dice_score = 2. * intersection / (cardinality + 1e-6)\n",
        "        return 1. - dice_score.mean()\n",
        "\n",
        "# Combined loss function\n",
        "class CombinedLoss(nn.Module):\n",
        "    def __init__(self, weight=None):\n",
        "        super(CombinedLoss, self).__init__()\n",
        "        self.ce_loss = nn.CrossEntropyLoss(weight=weight)  # Removed ignore_index\n",
        "        self.dice_loss = DiceLossCustom(weight=weight)    # Removed ignore_index\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        ce = self.ce_loss(inputs, targets)\n",
        "        dice = self.dice_loss(inputs, targets)\n",
        "        return ce + dice\n",
        "\n",
        "criterion = CombinedLoss(weight=class_weights)\n",
        "\n",
        "# Adjusted optimizer and learning rate\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "\n",
        "# Cosine Annealing Scheduler with Warm Restarts\n",
        "scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2, eta_min=1e-6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HNTHmWVwAdv",
        "outputId": "39c07b74-eb7c-49c5-e69d-331ee018d714"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class weights: tensor([0.3502, 1.0000, 6.8862], device='cuda:0')\n",
            "Class weights shape: torch.Size([3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Define IoU Metric\n",
        "We define the Intersection over Union (IoU) metric here, which measures segmentation accuracy by calculating overlap between predicted and true masks across classes.\n"
      ],
      "metadata": {
        "id": "uZkCvuDX47tK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# 9. Define IoU Metric\n",
        "# ============================\n",
        "\n",
        "def calculate_iou_per_class(preds, masks, num_classes=3):\n",
        "    \"\"\"\n",
        "    Calculates the Intersection over Union (IoU) for each class.\n",
        "\n",
        "    Args:\n",
        "        preds (torch.Tensor): Predicted segmentation maps of shape [batch_size, H, W].\n",
        "        masks (torch.Tensor): Ground truth segmentation maps of shape [batch_size, H, W].\n",
        "        num_classes (int): Number of classes.\n",
        "\n",
        "    Returns:\n",
        "        list: IoU for each class.\n",
        "    \"\"\"\n",
        "    ious = []\n",
        "    preds = preds.view(-1)\n",
        "    masks = masks.view(-1)\n",
        "    for cls in range(num_classes):\n",
        "        pred_inds = preds == cls\n",
        "        target_inds = masks == cls\n",
        "        intersection = (pred_inds & target_inds).sum().item()\n",
        "        union = pred_inds.sum().item() + target_inds.sum().item() - intersection\n",
        "        if union > 0:\n",
        "            ious.append(float(intersection) / float(union))\n",
        "        else:\n",
        "            ious.append(float('nan'))\n",
        "    return ious  # List containing IoU for each class\n",
        "\n",
        "metrics = {'iou_score': calculate_iou_per_class}"
      ],
      "metadata": {
        "id": "25sOoAFdwC4c"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10. Define Training Function\n",
        "The training function includes:\n",
        "- **Automatic Mixed Precision (AMP)**: AMP optimizes memory and computational efficiency, which is beneficial for training complex models like DeepLabV3+ on GPUs.\n",
        "- **Early Stopping**: Stops training when validation IoU no longer improves, preventing overfitting and unnecessary computation.\n",
        "- **Mean IoU Tracking**: Mean IoU and per-class IoU tracking help us gauge model performance across all classes. The model with the best validation IoU is saved, ensuring we retain the most performant model.\n"
      ],
      "metadata": {
        "id": "d07AFmz-5C5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# 10. Define Training Function\n",
        "# ============================\n",
        "\n",
        "def train_model(model, criterion, dataloaders, optimizer, metrics, num_epochs=50, patience=10):\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_mean_iou = 0.0\n",
        "    scaler = GradScaler()  # Updated GradScaler instantiation\n",
        "    epochs_no_improve = 0\n",
        "    since = time.time()\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        print(f'Epoch {epoch}/{num_epochs}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        for phase in ['Train', 'Val']:\n",
        "            if phase == 'Train':\n",
        "                model.train()\n",
        "                dataloader = dataloaders['Train']\n",
        "            else:\n",
        "                model.eval()\n",
        "                dataloader = dataloaders['Val']\n",
        "\n",
        "            running_loss = 0.0\n",
        "            all_ious = []\n",
        "\n",
        "            for batch_idx, (inputs, masks) in enumerate(dataloader):\n",
        "                inputs, masks = inputs.to(device), masks.to(device)\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'Train'):\n",
        "                    with autocast(device_type=device.type):  # Updated autocast usage\n",
        "                        outputs = model(inputs)\n",
        "                        loss = criterion(outputs, masks)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    ious = calculate_iou_per_class(preds, masks, num_classes=3)\n",
        "                    all_ious.append(ious)\n",
        "\n",
        "                    if phase == 'Train':\n",
        "                        scaler.scale(loss).backward()\n",
        "                        scaler.step(optimizer)\n",
        "                        scaler.update()\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloader.dataset)\n",
        "            all_ious = np.array(all_ious)\n",
        "            epoch_ious = np.nanmean(all_ious, axis=0)\n",
        "            mean_iou = np.nanmean(epoch_ious)\n",
        "\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Mean IoU: {mean_iou:.4f}')\n",
        "            print(f'Per-Class IoU: Background={epoch_ious[0]:.4f}, Ground={epoch_ious[1]:.4f}, Pallet={epoch_ious[2]:.4f}')\n",
        "\n",
        "            # Deep copy the model if we have a better mean IoU\n",
        "            if phase == 'Val':\n",
        "                scheduler.step()\n",
        "                if mean_iou > best_mean_iou:\n",
        "                    best_mean_iou = mean_iou\n",
        "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                    epochs_no_improve = 0\n",
        "                    print(f'Best Val Mean IoU updated to {best_mean_iou:.4f}')\n",
        "                else:\n",
        "                    epochs_no_improve += 1\n",
        "                    print(f'No improvement in Val Mean IoU for {epochs_no_improve} epochs.')\n",
        "\n",
        "        # Early stopping\n",
        "        if epochs_no_improve >= patience:\n",
        "            print(f'Early stopping triggered after {patience} epochs with no improvement.')\n",
        "            break\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'Best Validation Mean IoU: {best_mean_iou:.4f}')\n",
        "\n",
        "    # Load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "\n",
        "# ============================\n",
        "# 11. Train the Model\n",
        "# ============================\n",
        "\n",
        "trained_model = train_model(model, criterion, dataloaders, optimizer, metrics, num_epochs=50)\n",
        "\n",
        "# ============================\n",
        "# 12. Save the Model\n",
        "# ============================\n",
        "\n",
        "model_dir = os.path.join(SEG_DATASET_DIR, \"model\")\n",
        "if not os.path.exists(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "\n",
        "model_path = os.path.join(model_dir, \"best_deeplabv3plus_model.pth\")\n",
        "torch.save(trained_model.state_dict(), model_path)\n",
        "print(f'Model saved to {model_path}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6yMJ52hQh4t",
        "outputId": "fc115060-b09c-4fbd-981a-3999a60e2ff8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class weights: tensor([0.3504, 1.0000, 6.8352], device='cuda:0')\n",
            "Class weights shape: torch.Size([3])\n",
            "Epoch 1/50\n",
            "----------\n",
            "Train Loss: 1.1166 Mean IoU: 0.5317\n",
            "Per-Class IoU: Background=0.7466, Ground=0.7436, Pallet=0.1048\n",
            "Val Loss: 0.7625 Mean IoU: 0.6302\n",
            "Per-Class IoU: Background=0.8279, Ground=0.8944, Pallet=0.1684\n",
            "Best Val Mean IoU updated to 0.6302\n",
            "\n",
            "Epoch 2/50\n",
            "----------\n",
            "Train Loss: 0.8605 Mean IoU: 0.5830\n",
            "Per-Class IoU: Background=0.7670, Ground=0.8385, Pallet=0.1435\n",
            "Val Loss: 0.7209 Mean IoU: 0.5818\n",
            "Per-Class IoU: Background=0.7139, Ground=0.9097, Pallet=0.1217\n",
            "No improvement in Val Mean IoU for 1 epochs.\n",
            "\n",
            "Epoch 3/50\n",
            "----------\n",
            "Train Loss: 0.7609 Mean IoU: 0.6129\n",
            "Per-Class IoU: Background=0.7964, Ground=0.8728, Pallet=0.1694\n",
            "Val Loss: 0.6864 Mean IoU: 0.6045\n",
            "Per-Class IoU: Background=0.7540, Ground=0.9266, Pallet=0.1330\n",
            "No improvement in Val Mean IoU for 2 epochs.\n",
            "\n",
            "Epoch 4/50\n",
            "----------\n",
            "Train Loss: 0.6972 Mean IoU: 0.6356\n",
            "Per-Class IoU: Background=0.8212, Ground=0.8890, Pallet=0.1967\n",
            "Val Loss: 0.6244 Mean IoU: 0.6387\n",
            "Per-Class IoU: Background=0.8110, Ground=0.9361, Pallet=0.1688\n",
            "Best Val Mean IoU updated to 0.6387\n",
            "\n",
            "Epoch 5/50\n",
            "----------\n",
            "Train Loss: 0.6317 Mean IoU: 0.6552\n",
            "Per-Class IoU: Background=0.8416, Ground=0.8909, Pallet=0.2333\n",
            "Val Loss: 0.6023 Mean IoU: 0.6511\n",
            "Per-Class IoU: Background=0.8316, Ground=0.9324, Pallet=0.1894\n",
            "Best Val Mean IoU updated to 0.6511\n",
            "\n",
            "Epoch 6/50\n",
            "----------\n",
            "Train Loss: 0.7092 Mean IoU: 0.6411\n",
            "Per-Class IoU: Background=0.8270, Ground=0.8920, Pallet=0.2045\n",
            "Val Loss: 0.7107 Mean IoU: 0.6497\n",
            "Per-Class IoU: Background=0.8610, Ground=0.9161, Pallet=0.1721\n",
            "No improvement in Val Mean IoU for 1 epochs.\n",
            "\n",
            "Epoch 7/50\n",
            "----------\n",
            "Train Loss: 0.7381 Mean IoU: 0.6242\n",
            "Per-Class IoU: Background=0.8099, Ground=0.8814, Pallet=0.1814\n",
            "Val Loss: 0.6506 Mean IoU: 0.6597\n",
            "Per-Class IoU: Background=0.8558, Ground=0.9437, Pallet=0.1796\n",
            "Best Val Mean IoU updated to 0.6597\n",
            "\n",
            "Epoch 8/50\n",
            "----------\n",
            "Train Loss: 0.6564 Mean IoU: 0.6501\n",
            "Per-Class IoU: Background=0.8381, Ground=0.8899, Pallet=0.2223\n",
            "Val Loss: 0.6022 Mean IoU: 0.6525\n",
            "Per-Class IoU: Background=0.8374, Ground=0.9445, Pallet=0.1754\n",
            "No improvement in Val Mean IoU for 1 epochs.\n",
            "\n",
            "Epoch 9/50\n",
            "----------\n",
            "Train Loss: 0.5962 Mean IoU: 0.6751\n",
            "Per-Class IoU: Background=0.8627, Ground=0.9030, Pallet=0.2596\n",
            "Val Loss: 0.5965 Mean IoU: 0.6417\n",
            "Per-Class IoU: Background=0.8107, Ground=0.9492, Pallet=0.1651\n",
            "No improvement in Val Mean IoU for 2 epochs.\n",
            "\n",
            "Epoch 10/50\n",
            "----------\n",
            "Train Loss: 0.5706 Mean IoU: 0.6757\n",
            "Per-Class IoU: Background=0.8594, Ground=0.9084, Pallet=0.2592\n",
            "Val Loss: 0.5730 Mean IoU: 0.6623\n",
            "Per-Class IoU: Background=0.8433, Ground=0.9486, Pallet=0.1950\n",
            "Best Val Mean IoU updated to 0.6623\n",
            "\n",
            "Epoch 11/50\n",
            "----------\n",
            "Train Loss: 0.5262 Mean IoU: 0.6925\n",
            "Per-Class IoU: Background=0.8760, Ground=0.9163, Pallet=0.2852\n",
            "Val Loss: 0.5883 Mean IoU: 0.6794\n",
            "Per-Class IoU: Background=0.8733, Ground=0.9479, Pallet=0.2169\n",
            "Best Val Mean IoU updated to 0.6794\n",
            "\n",
            "Epoch 12/50\n",
            "----------\n",
            "Train Loss: 0.4814 Mean IoU: 0.7125\n",
            "Per-Class IoU: Background=0.8945, Ground=0.9194, Pallet=0.3237\n",
            "Val Loss: 0.5819 Mean IoU: 0.6874\n",
            "Per-Class IoU: Background=0.8834, Ground=0.9532, Pallet=0.2256\n",
            "Best Val Mean IoU updated to 0.6874\n",
            "\n",
            "Epoch 13/50\n",
            "----------\n",
            "Train Loss: 0.4461 Mean IoU: 0.7236\n",
            "Per-Class IoU: Background=0.8997, Ground=0.9234, Pallet=0.3477\n",
            "Val Loss: 0.5641 Mean IoU: 0.6892\n",
            "Per-Class IoU: Background=0.8830, Ground=0.9529, Pallet=0.2317\n",
            "Best Val Mean IoU updated to 0.6892\n",
            "\n",
            "Epoch 14/50\n",
            "----------\n",
            "Train Loss: 0.4226 Mean IoU: 0.7334\n",
            "Per-Class IoU: Background=0.9079, Ground=0.9287, Pallet=0.3635\n",
            "Val Loss: 0.5640 Mean IoU: 0.6880\n",
            "Per-Class IoU: Background=0.8805, Ground=0.9516, Pallet=0.2319\n",
            "No improvement in Val Mean IoU for 1 epochs.\n",
            "\n",
            "Epoch 15/50\n",
            "----------\n",
            "Train Loss: 0.4016 Mean IoU: 0.7431\n",
            "Per-Class IoU: Background=0.9143, Ground=0.9246, Pallet=0.3905\n",
            "Val Loss: 0.5821 Mean IoU: 0.6907\n",
            "Per-Class IoU: Background=0.8872, Ground=0.9510, Pallet=0.2340\n",
            "Best Val Mean IoU updated to 0.6907\n",
            "\n",
            "Epoch 16/50\n",
            "----------\n",
            "Train Loss: 0.5524 Mean IoU: 0.6825\n",
            "Per-Class IoU: Background=0.8643, Ground=0.9026, Pallet=0.2805\n",
            "Val Loss: 0.6395 Mean IoU: 0.6450\n",
            "Per-Class IoU: Background=0.8305, Ground=0.9291, Pallet=0.1754\n",
            "No improvement in Val Mean IoU for 1 epochs.\n",
            "\n",
            "Epoch 17/50\n",
            "----------\n",
            "Train Loss: 0.5991 Mean IoU: 0.6630\n",
            "Per-Class IoU: Background=0.8492, Ground=0.9010, Pallet=0.2386\n",
            "Val Loss: 0.6372 Mean IoU: 0.6626\n",
            "Per-Class IoU: Background=0.8575, Ground=0.9474, Pallet=0.1830\n",
            "No improvement in Val Mean IoU for 2 epochs.\n",
            "\n",
            "Epoch 18/50\n",
            "----------\n",
            "Train Loss: 0.5761 Mean IoU: 0.6780\n",
            "Per-Class IoU: Background=0.8618, Ground=0.9045, Pallet=0.2676\n",
            "Val Loss: 0.6321 Mean IoU: 0.6725\n",
            "Per-Class IoU: Background=0.8797, Ground=0.9201, Pallet=0.2178\n",
            "No improvement in Val Mean IoU for 3 epochs.\n",
            "\n",
            "Epoch 19/50\n",
            "----------\n",
            "Train Loss: 0.5258 Mean IoU: 0.6931\n",
            "Per-Class IoU: Background=0.8797, Ground=0.9058, Pallet=0.2938\n",
            "Val Loss: 0.6081 Mean IoU: 0.6747\n",
            "Per-Class IoU: Background=0.8787, Ground=0.9249, Pallet=0.2206\n",
            "No improvement in Val Mean IoU for 4 epochs.\n",
            "\n",
            "Epoch 20/50\n",
            "----------\n",
            "Train Loss: 0.4944 Mean IoU: 0.7032\n",
            "Per-Class IoU: Background=0.8851, Ground=0.9143, Pallet=0.3103\n",
            "Val Loss: 0.6440 Mean IoU: 0.6990\n",
            "Per-Class IoU: Background=0.8979, Ground=0.9586, Pallet=0.2404\n",
            "Best Val Mean IoU updated to 0.6990\n",
            "\n",
            "Epoch 21/50\n",
            "----------\n",
            "Train Loss: 0.4843 Mean IoU: 0.7041\n",
            "Per-Class IoU: Background=0.8849, Ground=0.9181, Pallet=0.3094\n",
            "Val Loss: 0.6670 Mean IoU: 0.6630\n",
            "Per-Class IoU: Background=0.8597, Ground=0.9464, Pallet=0.1828\n",
            "No improvement in Val Mean IoU for 1 epochs.\n",
            "\n",
            "Epoch 22/50\n",
            "----------\n",
            "Train Loss: 0.4801 Mean IoU: 0.7058\n",
            "Per-Class IoU: Background=0.8881, Ground=0.9180, Pallet=0.3114\n",
            "Val Loss: 0.6088 Mean IoU: 0.6733\n",
            "Per-Class IoU: Background=0.8711, Ground=0.9423, Pallet=0.2064\n",
            "No improvement in Val Mean IoU for 2 epochs.\n",
            "\n",
            "Epoch 23/50\n",
            "----------\n",
            "Train Loss: 0.4171 Mean IoU: 0.7341\n",
            "Per-Class IoU: Background=0.9077, Ground=0.9240, Pallet=0.3705\n",
            "Val Loss: 0.6200 Mean IoU: 0.6984\n",
            "Per-Class IoU: Background=0.8999, Ground=0.9501, Pallet=0.2452\n",
            "No improvement in Val Mean IoU for 3 epochs.\n",
            "\n",
            "Epoch 24/50\n",
            "----------\n",
            "Train Loss: 0.4297 Mean IoU: 0.7247\n",
            "Per-Class IoU: Background=0.9027, Ground=0.9224, Pallet=0.3491\n",
            "Val Loss: 0.6373 Mean IoU: 0.6841\n",
            "Per-Class IoU: Background=0.8864, Ground=0.9357, Pallet=0.2303\n",
            "No improvement in Val Mean IoU for 4 epochs.\n",
            "\n",
            "Epoch 25/50\n",
            "----------\n",
            "Train Loss: 0.3794 Mean IoU: 0.7503\n",
            "Per-Class IoU: Background=0.9175, Ground=0.9285, Pallet=0.4050\n",
            "Val Loss: 0.5963 Mean IoU: 0.7109\n",
            "Per-Class IoU: Background=0.9111, Ground=0.9422, Pallet=0.2795\n",
            "Best Val Mean IoU updated to 0.7109\n",
            "\n",
            "Epoch 26/50\n",
            "----------\n",
            "Train Loss: 0.3631 Mean IoU: 0.7582\n",
            "Per-Class IoU: Background=0.9230, Ground=0.9337, Pallet=0.4179\n",
            "Val Loss: 0.5933 Mean IoU: 0.6964\n",
            "Per-Class IoU: Background=0.8972, Ground=0.9383, Pallet=0.2537\n",
            "No improvement in Val Mean IoU for 1 epochs.\n",
            "\n",
            "Epoch 27/50\n",
            "----------\n",
            "Train Loss: 0.3310 Mean IoU: 0.7700\n",
            "Per-Class IoU: Background=0.9297, Ground=0.9402, Pallet=0.4401\n",
            "Val Loss: 0.6213 Mean IoU: 0.7009\n",
            "Per-Class IoU: Background=0.9014, Ground=0.9471, Pallet=0.2542\n",
            "No improvement in Val Mean IoU for 2 epochs.\n",
            "\n",
            "Epoch 28/50\n",
            "----------\n",
            "Train Loss: 0.3021 Mean IoU: 0.7860\n",
            "Per-Class IoU: Background=0.9362, Ground=0.9454, Pallet=0.4764\n",
            "Val Loss: 0.6649 Mean IoU: 0.7093\n",
            "Per-Class IoU: Background=0.9107, Ground=0.9475, Pallet=0.2695\n",
            "No improvement in Val Mean IoU for 3 epochs.\n",
            "\n",
            "Epoch 29/50\n",
            "----------\n",
            "Train Loss: 0.3127 Mean IoU: 0.7875\n",
            "Per-Class IoU: Background=0.9376, Ground=0.9426, Pallet=0.4822\n",
            "Val Loss: 0.6937 Mean IoU: 0.7049\n",
            "Per-Class IoU: Background=0.9132, Ground=0.9345, Pallet=0.2670\n",
            "No improvement in Val Mean IoU for 4 epochs.\n",
            "\n",
            "Epoch 30/50\n",
            "----------\n",
            "Train Loss: 0.2805 Mean IoU: 0.7967\n",
            "Per-Class IoU: Background=0.9413, Ground=0.9444, Pallet=0.5042\n",
            "Val Loss: 0.6778 Mean IoU: 0.7053\n",
            "Per-Class IoU: Background=0.9116, Ground=0.9361, Pallet=0.2681\n",
            "No improvement in Val Mean IoU for 5 epochs.\n",
            "\n",
            "Epoch 31/50\n",
            "----------\n",
            "Train Loss: 0.2806 Mean IoU: 0.8040\n",
            "Per-Class IoU: Background=0.9443, Ground=0.9451, Pallet=0.5224\n",
            "Val Loss: 0.6587 Mean IoU: 0.7023\n",
            "Per-Class IoU: Background=0.9069, Ground=0.9384, Pallet=0.2615\n",
            "No improvement in Val Mean IoU for 6 epochs.\n",
            "\n",
            "Epoch 32/50\n",
            "----------\n",
            "Train Loss: 0.2772 Mean IoU: 0.7978\n",
            "Per-Class IoU: Background=0.9424, Ground=0.9423, Pallet=0.5088\n",
            "Val Loss: 0.6962 Mean IoU: 0.7065\n",
            "Per-Class IoU: Background=0.9153, Ground=0.9334, Pallet=0.2708\n",
            "No improvement in Val Mean IoU for 7 epochs.\n",
            "\n",
            "Epoch 33/50\n",
            "----------\n",
            "Train Loss: 0.2587 Mean IoU: 0.8113\n",
            "Per-Class IoU: Background=0.9460, Ground=0.9487, Pallet=0.5393\n",
            "Val Loss: 0.6879 Mean IoU: 0.7111\n",
            "Per-Class IoU: Background=0.9166, Ground=0.9380, Pallet=0.2787\n",
            "Best Val Mean IoU updated to 0.7111\n",
            "\n",
            "Epoch 34/50\n",
            "----------\n",
            "Train Loss: 0.2602 Mean IoU: 0.8109\n",
            "Per-Class IoU: Background=0.9480, Ground=0.9481, Pallet=0.5365\n",
            "Val Loss: 0.7099 Mean IoU: 0.7131\n",
            "Per-Class IoU: Background=0.9187, Ground=0.9422, Pallet=0.2784\n",
            "Best Val Mean IoU updated to 0.7131\n",
            "\n",
            "Epoch 35/50\n",
            "----------\n",
            "Train Loss: 0.2571 Mean IoU: 0.8078\n",
            "Per-Class IoU: Background=0.9476, Ground=0.9495, Pallet=0.5263\n",
            "Val Loss: 0.6969 Mean IoU: 0.7048\n",
            "Per-Class IoU: Background=0.9118, Ground=0.9364, Pallet=0.2662\n",
            "No improvement in Val Mean IoU for 1 epochs.\n",
            "\n",
            "Epoch 36/50\n",
            "----------\n",
            "Train Loss: 0.3733 Mean IoU: 0.7552\n",
            "Per-Class IoU: Background=0.9155, Ground=0.9298, Pallet=0.4203\n",
            "Val Loss: 0.6717 Mean IoU: 0.7010\n",
            "Per-Class IoU: Background=0.9007, Ground=0.9258, Pallet=0.2764\n",
            "No improvement in Val Mean IoU for 2 epochs.\n",
            "\n",
            "Epoch 37/50\n",
            "----------\n",
            "Train Loss: 0.4903 Mean IoU: 0.7053\n",
            "Per-Class IoU: Background=0.8846, Ground=0.9127, Pallet=0.3184\n",
            "Val Loss: 0.6548 Mean IoU: 0.6707\n",
            "Per-Class IoU: Background=0.8615, Ground=0.9488, Pallet=0.2017\n",
            "No improvement in Val Mean IoU for 3 epochs.\n",
            "\n",
            "Epoch 38/50\n",
            "----------\n",
            "Train Loss: 0.4347 Mean IoU: 0.7217\n",
            "Per-Class IoU: Background=0.8984, Ground=0.9130, Pallet=0.3537\n",
            "Val Loss: 0.6159 Mean IoU: 0.6895\n",
            "Per-Class IoU: Background=0.8892, Ground=0.9256, Pallet=0.2538\n",
            "No improvement in Val Mean IoU for 4 epochs.\n",
            "\n",
            "Epoch 39/50\n",
            "----------\n",
            "Train Loss: 0.4944 Mean IoU: 0.7031\n",
            "Per-Class IoU: Background=0.8867, Ground=0.9107, Pallet=0.3120\n",
            "Val Loss: 0.6132 Mean IoU: 0.6523\n",
            "Per-Class IoU: Background=0.8341, Ground=0.9384, Pallet=0.1844\n",
            "No improvement in Val Mean IoU for 5 epochs.\n",
            "\n",
            "Epoch 40/50\n",
            "----------\n",
            "Train Loss: 0.4222 Mean IoU: 0.7266\n",
            "Per-Class IoU: Background=0.9034, Ground=0.9248, Pallet=0.3516\n",
            "Val Loss: 0.6155 Mean IoU: 0.6767\n",
            "Per-Class IoU: Background=0.8651, Ground=0.9477, Pallet=0.2174\n",
            "No improvement in Val Mean IoU for 6 epochs.\n",
            "\n",
            "Epoch 41/50\n",
            "----------\n",
            "Train Loss: 0.3890 Mean IoU: 0.7446\n",
            "Per-Class IoU: Background=0.9140, Ground=0.9280, Pallet=0.3919\n",
            "Val Loss: 0.6445 Mean IoU: 0.6905\n",
            "Per-Class IoU: Background=0.8869, Ground=0.9577, Pallet=0.2269\n",
            "No improvement in Val Mean IoU for 7 epochs.\n",
            "\n",
            "Epoch 42/50\n",
            "----------\n",
            "Train Loss: 0.3897 Mean IoU: 0.7474\n",
            "Per-Class IoU: Background=0.9162, Ground=0.9304, Pallet=0.3957\n",
            "Val Loss: 0.6660 Mean IoU: 0.6847\n",
            "Per-Class IoU: Background=0.8913, Ground=0.9405, Pallet=0.2223\n",
            "No improvement in Val Mean IoU for 8 epochs.\n",
            "\n",
            "Epoch 43/50\n",
            "----------\n",
            "Train Loss: 0.3895 Mean IoU: 0.7511\n",
            "Per-Class IoU: Background=0.9153, Ground=0.9301, Pallet=0.4079\n",
            "Val Loss: 0.6434 Mean IoU: 0.7029\n",
            "Per-Class IoU: Background=0.9044, Ground=0.9540, Pallet=0.2504\n",
            "No improvement in Val Mean IoU for 9 epochs.\n",
            "\n",
            "Epoch 44/50\n",
            "----------\n",
            "Train Loss: 0.4050 Mean IoU: 0.7382\n",
            "Per-Class IoU: Background=0.9107, Ground=0.9205, Pallet=0.3836\n",
            "Val Loss: 0.6132 Mean IoU: 0.6948\n",
            "Per-Class IoU: Background=0.8954, Ground=0.9523, Pallet=0.2367\n",
            "No improvement in Val Mean IoU for 10 epochs.\n",
            "Early stopping triggered after 10 epochs with no improvement.\n",
            "Training complete in 15m 33s\n",
            "Best Validation Mean IoU: 0.7131\n",
            "Model saved to /content/drive/MyDrive/Pallets_detection/Deeplabv3_ObjectSegmentation_Dataset/model/best_deeplabv3plus_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 13. Post-Processing Function\n",
        "This function applies morphological operations (opening and closing) to clean up segmentation masks, improving mask quality by smoothing boundaries.\n",
        "\n",
        "### 14. Analyze Predictions\n",
        "Finally, we visualize model predictions by decoding segmentation masks into color-coded images, comparing them to ground truth masks to assess performance."
      ],
      "metadata": {
        "id": "z7KlpGSi5R-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# 13. Post-Processing Function\n",
        "# ============================\n",
        "\n",
        "import cv2\n",
        "\n",
        "def post_process(mask):\n",
        "    \"\"\"\n",
        "    Apply morphological operations to clean up the mask.\n",
        "\n",
        "    Args:\n",
        "        mask (numpy.ndarray): Predicted mask.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Post-processed mask.\n",
        "    \"\"\"\n",
        "    mask = mask.astype(np.uint8)\n",
        "    kernel = np.ones((5, 5), np.uint8)\n",
        "    # Opening (erosion followed by dilation)\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
        "    # Closing (dilation followed by erosion)\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
        "    return mask\n",
        "\n",
        "# ============================\n",
        "# 14. Analyze Predictions\n",
        "# ============================\n",
        "\n",
        "def decode_segmap(image, num_classes):\n",
        "    \"\"\"\n",
        "    Decodes a segmentation mask to an RGB image.\n",
        "\n",
        "    Args:\n",
        "        image (numpy.ndarray): Segmentation mask of shape [H, W].\n",
        "        num_classes (int): Number of classes.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: RGB image of shape [H, W, 3].\n",
        "    \"\"\"\n",
        "    label_colors = np.array([\n",
        "        (0, 0, 0),          # Background - Black\n",
        "        (128, 128, 128),    # Ground - Gray\n",
        "        (255, 255, 255)     # Pallet - White\n",
        "    ])\n",
        "    r, g, b = [np.zeros_like(image).astype(np.uint8) for _ in range(3)]\n",
        "    for l in range(num_classes):\n",
        "        idx = image == l\n",
        "        r[idx], g[idx], b[idx] = label_colors[l]\n",
        "    return np.stack([r, g, b], axis=2)\n",
        "\n",
        "# Load the best model weights\n",
        "trained_model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "trained_model.eval()\n",
        "print(f'Model loaded from {model_path}')\n",
        "\n",
        "# Define directory to save plots\n",
        "save_dir = \"/content/segmentation_results\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for idx, (input, mask) in enumerate(test_loader):\n",
        "        input, mask = input.to(device), mask.to(device)\n",
        "        output = trained_model(input)\n",
        "        pred = torch.argmax(output, 1).cpu().squeeze().numpy()\n",
        "\n",
        "        # Post-process the prediction\n",
        "        pred = post_process(pred)\n",
        "\n",
        "        input_np = input.cpu().squeeze().permute(1, 2, 0).numpy()\n",
        "        # Denormalize\n",
        "        input_np = (input_np * np.array([0.229, 0.224, 0.225])) + np.array([0.485, 0.456, 0.406])\n",
        "        input_np = np.clip(input_np, 0, 1)\n",
        "\n",
        "        mask_np, pred_np = mask.cpu().squeeze().numpy(), pred\n",
        "\n",
        "        mask_rgb = decode_segmap(mask_np, num_classes=3)\n",
        "        pred_rgb = decode_segmap(pred_np, num_classes=3)\n",
        "\n",
        "        # Create the plot\n",
        "        fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
        "        axs[0].imshow(input_np)\n",
        "        axs[0].set_title('Input Image')\n",
        "        axs[0].axis('off')\n",
        "\n",
        "        axs[1].imshow(mask_rgb)\n",
        "        axs[1].set_title('Ground Truth')\n",
        "        axs[1].axis('off')\n",
        "\n",
        "        axs[2].imshow(pred_rgb)\n",
        "        axs[2].set_title('Predicted Segmentation')\n",
        "        axs[2].axis('off')\n",
        "\n",
        "        # Save the plot as an image file\n",
        "        plot_path = os.path.join(save_dir, f\"segmentation_result_{idx + 1}.png\")\n",
        "        plt.savefig(plot_path)\n",
        "        print(f\"Saved: {plot_path}\")\n",
        "\n",
        "        plt.close(fig)\n",
        "\n",
        "        if idx >= 4:  # Only process first 5 images\n",
        "            break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YseRYgd0oUUN",
        "outputId": "7abbfd21-650a-4b8d-c8b9-e5405bd5f933"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-7f951e28bb53>:52: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  trained_model.load_state_dict(torch.load(model_path, map_location=device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded from /content/drive/MyDrive/Pallets_detection/Deeplabv3_ObjectSegmentation_Dataset/model/best_deeplabv3plus_model.pth\n",
            "Saved: /content/segmentation_results/segmentation_result_1.png\n",
            "Saved: /content/segmentation_results/segmentation_result_2.png\n",
            "Saved: /content/segmentation_results/segmentation_result_3.png\n",
            "Saved: /content/segmentation_results/segmentation_result_4.png\n",
            "Saved: /content/segmentation_results/segmentation_result_5.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yFCoajHHudQl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}